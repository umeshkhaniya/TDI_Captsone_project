{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data in public S3 bucket('s3://nrel-pds-nsrdb/v3/nsrdb_2013.h5') is 1.6TB. In order to ingest the data into spark, partitions of 1000 is done and only required column is selected. The data is then saved in parquet file in S3 bucket.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the spark session\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName('solar')\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import os,sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext, DataFrame\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import s3fs\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file from Bucket\n",
    "s3_input_solar = 's3://nrel-pds-nsrdb/v3/nsrdb_2013.h5'\n",
    "s3_input_solar = 's3://nrel-pds-nsrdb/v3/puerto_rico/nsrdb_puerto_rico_2013.h5'\n",
    "output_to_save_nrel = 's3://data-input-captsone/input-nrel/input-nrel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the ghi\n",
    "_h5file = h5py.File(s3_input_solar, 'r')\n",
    "list(_h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will read the .h5 file from bucket. The file size is 1.5 TB and do processing in same in the Parquet format.\n",
    "def Hdf5ToParquet(filepath, output_path, spark):\n",
    "    meta_cols=['latitude','longitude']\n",
    "    ghi_cols=['ghi']\n",
    "    meta='meta' # store the geospatial info\n",
    "    ghi='ghi' # dataset name of ghi: store the ghi (irradiation) infor\n",
    "    # read the file\n",
    "    s3=s3fs.S3FileSystem(anon=False)\n",
    "    _h5file= h5py.File(s3.open(filepath), \"r\")\n",
    "    file_size=_h5file[meta].shape[0]\n",
    "    scale_factor=_h5file[ghi].attrs['psm_scale_factor']\n",
    "     #divide into chunk b/c file is too large 1.5 TB\n",
    "    chunk_size = 10000\n",
    "    n_chunk= file_size//chunk_size+1\n",
    "    \n",
    "    def readchunk(v):\n",
    "        s3=s3fs.S3FileSystem(anon=False)\n",
    "        _h5file= h5py.File(s3.open(filepath), \"r\")\n",
    "        meta_chunk=_h5file[meta][v*chunk_size:(v+1)*chunk_size]\n",
    "        ghi_chunk=_h5file[ghi][:,v*chunk_size:(v+1)*chunk_size].sum(axis=0)/scale_factor \n",
    "        def convertNptype(nl):  \n",
    "            \"\"\" \n",
    "            converting np data type to python type \n",
    "            since spark rdd doesn't know numpy\n",
    "            \"\"\"\n",
    "            res=getattr(nl, \"tolist\", lambda: value)()\n",
    "            return res\n",
    "        meta_l=  [convertNptype(meta_chunk[x]) for x in meta_cols] # only meta_cols\n",
    "        ghi_l = convertNptype(ghi_chunk)\n",
    "        agg_l= meta_l+[ghi_l]\n",
    "        return list(map(list, zip(*agg_l)))\n",
    "    \n",
    "    df=(spark.sparkContext.parallelize(range(0,n_chunk))\n",
    "        .flatMap(lambda v: readchunk(v))\n",
    "        .toDF(meta_cols+ghi_cols))\n",
    "    # convert \n",
    "    df.write.mode(\"overwrite\").parquet(f\"{output_path}.parquet\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the file\n",
    "Hdf5ToParquet(input_file, output_to_save_nrel, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
